# @generated from torch/_C/_VariableFunctions.pyi.in

from torch import Tensor, Generator, strided, memory_format, contiguous_format
from typing import List, Tuple, Optional, Union, Any, ContextManager, Callable, overload, Iterator, NamedTuple, Sequence, TypeVar
from torch._six import inf

from torch.types import _int, _float, _bool, Number, _dtype, _device, _qscheme, _size, _layout

import builtins

# REDUNDANT!
namedtuple_values_indices = NamedTuple("namedtuple_values_indices", [("values", Tensor), ("indices", Tensor)])
namedtuple_eigenvalues_eigenvectors = NamedTuple("namedtuple_eigenvalues_eigenvectors", [("eigenvalues", Tensor), ("eigenvectors", Tensor)])
namedtuple_a_tau = NamedTuple("namedtuple_a_tau", [("a", Tensor), ("tau", Tensor)])
namedtuple_solution_QR = NamedTuple("namedtuple_solution_QR", [("solution", Tensor), ("QR", Tensor)])
namedtuple_Q_R = NamedTuple("namedtuple_Q_R", [("Q", Tensor), ("R", Tensor)])
namedtuple_sign_logabsdet = NamedTuple("namedtuple_sign_logabsdet", [("sign", Tensor), ("logabsdet", Tensor)])
namedtuple_solution_LU = NamedTuple("namedtuple_solution_LU", [("solution", Tensor), ("LU", Tensor)])
namedtuple_U_S_V = NamedTuple("namedtuple_U_S_V", [("U", Tensor), ("S", Tensor), ("V", Tensor)])
namedtuple_solution_cloned_coefficient = NamedTuple("namedtuple_solution_cloned_coefficient", [("solution", Tensor), ("cloned_coefficient", Tensor)])

@overload
def __and__(self: Tensor, other: Number) -> Tensor: ...
@overload
def __and__(self: Tensor, other: Tensor) -> Tensor: ...
@overload
def __lshift__(self: Tensor, other: Number) -> Tensor: ...
@overload
def __lshift__(self: Tensor, other: Tensor) -> Tensor: ...
@overload
def __or__(self: Tensor, other: Number) -> Tensor: ...
@overload
def __or__(self: Tensor, other: Tensor) -> Tensor: ...
@overload
def __rshift__(self: Tensor, other: Number) -> Tensor: ...
@overload
def __rshift__(self: Tensor, other: Tensor) -> Tensor: ...
@overload
def __xor__(self: Tensor, other: Number) -> Tensor: ...
@overload
def __xor__(self: Tensor, other: Tensor) -> Tensor: ...
def _adaptive_avg_pool2d(self: Tensor, output_size: Union[_int, _size]) -> Tensor: ...
def _addmv_impl_(self: Tensor, self2: Tensor, mat: Tensor, vec: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def _addr(self: Tensor, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
def _addr_(self: Tensor, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def _amp_non_finite_check_and_unscale_(self: Tensor, found_inf: Tensor, inv_scale: Tensor) -> None: ...
def _amp_update_scale(growth_tracker: Tensor, current_scale: Tensor, found_inf: Tensor, scale_growth_factor: _float, scale_backoff_factor: _float, growth_interval: _int) -> Tensor: ...
def _baddbmm_mkl_(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
def _batch_norm_impl_index(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tuple[Tensor, Tensor, Tensor, Tensor, _int]: ...
def _bmm(self: Tensor, mat2: Tensor, *, deterministic: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...
def _cast_Byte(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Char(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Double(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Float(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Half(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Int(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Long(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cast_Short(self: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...
def _choose_qparams_per_tensor(self: Tensor, reduce_range: _bool=False) -> Tuple[_float, _int]: ...
def _convolution(input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size, groups: _int, benchmark: _bool, deterministic: _bool, cudnn_enabled: _bool) -> Tensor: ...
def _convolution_nogroup(input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size) -> Tensor: ...
def _copy_from(self: Tensor, dst: Tensor, non_blocking: _bool=False) -> Tensor: ...
def _ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: _size, target_lengths: _size, blank: _int=0, zero_infinity: _bool=False) -> Tuple[Tensor, Tensor]: ...
def _cudnn_ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: _size, target_lengths: _size, blank: _int, deterministic: _bool, zero_infinity: _bool) -> Tuple[Tensor, Tensor]: ...
def _cudnn_init_dropout_state(dropout: _float, train: _bool, dropout_seed: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def _cudnn_rnn(input: Tensor, weight: Union[Tuple[Tensor, ...], List[Tensor]], weight_stride0: _int, weight_buf: Optional[Tensor], hx: Tensor, cx: Optional[Tensor], mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, dropout: _float, train: _bool, bidirectional: _bool, batch_sizes: _size, dropout_state: Optional[Tensor]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def _cudnn_rnn_flatten_weight(weight_arr: Union[Tuple[Tensor, ...], List[Tensor]], weight_stride0: _int, input_size: _int, mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, bidirectional: _bool) -> Tensor: ...
def _cufft_clear_plan_cache(device_index: _int) -> None: ...
def _cufft_get_plan_cache_max_size(device_index: _int) -> _int: ...
def _cufft_get_plan_cache_size(device_index: _int) -> _int: ...
def _cufft_set_plan_cache_max_size(device_index: _int, max_size: _int) -> None: ...
def _cummax_helper(self: Tensor, values: Tensor, indices: Tensor, dim: _int) -> None: ...
def _cummin_helper(self: Tensor, values: Tensor, indices: Tensor, dim: _int) -> None: ...
def _debug_has_internal_overlap(self: Tensor) -> _int: ...
def _dim_arange(like: Tensor, dim: _int) -> Tensor: ...
def _dirichlet_grad(x: Tensor, alpha: Tensor, total: Tensor) -> Tensor: ...
def _embedding_bag(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: _bool=False, mode: _int=0, sparse: _bool=False, per_sample_weights: Optional[Tensor]=None, include_last_offset: _bool=False) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...
@overload
def _empty_affine_quantized(size: _size, *, scale: _float=1, zero_point: _int=0, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def _empty_affine_quantized(*size: _int, scale: _float=1, zero_point: _int=0, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def _empty_per_channel_affine_quantized(size: _size, *, scales: Tensor, zero_points: Tensor, axis: _int, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def _empty_per_channel_affine_quantized(*size: _int, scales: Tensor, zero_points: Tensor, axis: _int, memory_format: Optional[memory_format]=contiguous_format, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def _euclidean_dist(x1: Tensor, x2: Tensor) -> Tensor: ...
def _fft_with_size(self: Tensor, signal_ndim: _int, complex_input: _bool, complex_output: _bool, inverse: _bool, checked_signal_sizes: _size, normalized: _bool, onesided: _bool, output_sizes: _size) -> Tensor: ...
def _fused_dropout(self: Tensor, p: _float, generator: Optional[Generator]=None) -> Tuple[Tensor, Tensor]: ...
def _has_compatible_shallow_copy_type(self: Tensor, from_: Tensor) -> _bool: ...
def _index_copy_(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...
def _index_put_impl_(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False, unsafe: _bool=False) -> Tensor: ...
def _log_softmax(self: Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...
def _log_softmax_backward_data(grad_output: Tensor, output: Tensor, dim: _int, self: Tensor) -> Tensor: ...
def _logcumsumexp(self: Tensor, dim: _int, *, out: Optional[Tensor]=None) -> Tensor: ...
def _lu_solve_helper(self: Tensor, LU_data: Tensor, LU_pivots: Tensor) -> Tensor: ...
def _lu_with_info(self: Tensor, pivot: _bool=True, check_errors: _bool=True) -> Tuple[Tensor, Tensor, Tensor]: ...
def _make_per_channel_quantized_tensor(self: Tensor, scale: Tensor, zero_point: Tensor, axis: _int) -> Tensor: ...
def _make_per_tensor_quantized_tensor(self: Tensor, scale: _float, zero_point: _int) -> Tensor: ...
def _masked_scale(self: Tensor, mask: Tensor, scale: _float) -> Tensor: ...
def _mkldnn_reshape(self: Tensor, shape: _size) -> Tensor: ...
def _mkldnn_transpose(self: Tensor, dim0: _int, dim1: _int) -> Tensor: ...
def _mkldnn_transpose_(self: Tensor, dim0: _int, dim1: _int) -> Tensor: ...
def _mode(self: Tensor, dim: _int=-1, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...
def _multinomial_alias_draw(J: Tensor, q: Tensor, num_samples: _int, *, generator: Optional[Generator]=None) -> Tensor: ...
def _multinomial_alias_setup(probs: Tensor) -> Tuple[Tensor, Tensor]: ...
def _nnpack_available() -> _bool: ...
def _nnpack_spatial_convolution(input: Tensor, weight: Tensor, bias: Optional[Tensor], padding: Union[_int, _size], stride: Union[_int, _size]=1) -> Tensor: ...
def _pack_padded_sequence(input: Tensor, lengths: Tensor, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...
def _pad_packed_sequence(data: Tensor, batch_sizes: Tensor, batch_first: _bool, padding_value: Number, total_length: _int) -> Tuple[Tensor, Tensor]: ...
def _reshape_from_tensor(self: Tensor, shape: Tensor) -> Tensor: ...
def _s_where(condition: Tensor, self: Tensor, other: Tensor) -> Tensor: ...
def _sample_dirichlet(self: Tensor, generator: Optional[Generator]=None) -> Tensor: ...
def _shape_as_tensor(self: Tensor) -> Tensor: ...
def _sobol_engine_draw(quasi: Tensor, n: _int, sobolstate: Tensor, dimension: _int, num_generated: _int, dtype: Optional[_dtype]) -> Tuple[Tensor, Tensor]: ...
def _sobol_engine_ff_(self: Tensor, n: _int, sobolstate: Tensor, dimension: _int, num_generated: _int) -> Tensor: ...
def _sobol_engine_initialize_state_(self: Tensor, dimension: _int) -> Tensor: ...
def _sobol_engine_scramble_(self: Tensor, ltm: Tensor, dimension: _int) -> Tensor: ...
def _softmax(self: Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...
def _softmax_backward_data(grad_output: Tensor, output: Tensor, dim: _int, self: Tensor) -> Tensor: ...
def _sparse_addmm(self: Tensor, sparse: Tensor, dense: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
@overload
def _sparse_log_softmax(self: Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
@overload
def _sparse_log_softmax(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...
@overload
def _sparse_log_softmax(self: Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...
def _sparse_log_softmax_backward_data(grad_output: Tensor, output: Tensor, dim: _int, self: Tensor) -> Tensor: ...
def _sparse_mm(sparse: Tensor, dense: Tensor) -> Tensor: ...
@overload
def _sparse_softmax(self: Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
@overload
def _sparse_softmax(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...
@overload
def _sparse_softmax(self: Tensor, dim: _int, half_to_float: _bool) -> Tensor: ...
def _sparse_softmax_backward_data(grad_output: Tensor, output: Tensor, dim: _int, self: Tensor) -> Tensor: ...
@overload
def _sparse_sum(self: Tensor) -> Tensor: ...
@overload
def _sparse_sum(self: Tensor, *, dtype: _dtype) -> Tensor: ...
@overload
def _sparse_sum(self: Tensor, dim: Union[_int, _size]) -> Tensor: ...
@overload
def _sparse_sum(self: Tensor, dim: Union[_int, _size], *, dtype: _dtype) -> Tensor: ...
def _standard_gamma(self: Tensor, generator: Optional[Generator]=None) -> Tensor: ...
def _standard_gamma_grad(self: Tensor, output: Tensor) -> Tensor: ...
def _test_serialization_subcmul(self: Tensor, other: Tensor, alpha: Number=1) -> Tensor: ...
def _trilinear(i1: Tensor, i2: Tensor, i3: Tensor, expand1: _size, expand2: _size, expand3: _size, sumdim: _size, unroll_dim: _int=1) -> Tensor: ...
def _unique(self: Tensor, sorted: _bool=True, return_inverse: _bool=False) -> Tuple[Tensor, Tensor]: ...
def _unique2(self: Tensor, sorted: _bool=True, return_inverse: _bool=False, return_counts: _bool=False) -> Tuple[Tensor, Tensor, Tensor]: ...
def _use_cudnn_ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: _size, target_lengths: _size, blank: _int) -> _bool: ...
def _use_cudnn_rnn_flatten_weight() -> _bool: ...
def _weight_norm(v: Tensor, g: Tensor, dim: _int=0) -> Tensor: ...
def _weight_norm_cuda_interface(v: Tensor, g: Tensor, dim: _int=0) -> Tuple[Tensor, Tensor]: ...
def abs(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def abs_(self: Tensor) -> Tensor: ...
def absolute(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def absolute_(self: Tensor) -> Tensor: ...
def acos(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def acos_(self: Tensor) -> Tensor: ...
def acosh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def acosh_(self: Tensor) -> Tensor: ...
def adaptive_avg_pool1d(self: Tensor, output_size: Union[_int, _size]) -> Tensor: ...
def adaptive_max_pool1d(self: Tensor, output_size: Union[_int, _size]) -> Tuple[Tensor, Tensor]: ...
@overload
def add(input: Union[Tensor, Number], other: Union[Tensor, Number], *, alpha: Optional[Number]=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def add(self: Tensor, alpha: Number, other: Tensor) -> Tensor: ...
@overload
def add(self: Tensor, alpha: Number, other: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addbmm(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def addbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor) -> Tensor: ...
@overload
def addbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor) -> Tensor: ...
@overload
def addbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addcdiv(self: Tensor, tensor1: Tensor, tensor2: Tensor, *, value: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def addcdiv(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor) -> Tensor: ...
@overload
def addcdiv(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addcmul(self: Tensor, tensor1: Tensor, tensor2: Tensor, *, value: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def addcmul(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor) -> Tensor: ...
@overload
def addcmul(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addmm(self: Tensor, mat1: Tensor, mat2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def addmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor) -> Tensor: ...
@overload
def addmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor) -> Tensor: ...
@overload
def addmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addmv(self: Tensor, mat: Tensor, vec: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def addmv(beta: Number, self: Tensor, alpha: Number, mat: Tensor, vec: Tensor) -> Tensor: ...
@overload
def addmv(beta: Number, self: Tensor, alpha: Number, mat: Tensor, vec: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addmv(beta: Number, self: Tensor, mat: Tensor, vec: Tensor) -> Tensor: ...
@overload
def addmv(beta: Number, self: Tensor, mat: Tensor, vec: Tensor, *, out: Tensor) -> Tensor: ...
def addmv_(self: Tensor, mat: Tensor, vec: Tensor, *, beta: Number=1, alpha: Number=1) -> Tensor: ...
@overload
def addr(self: Tensor, vec1: Tensor, vec2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def addr(beta: Number, self: Tensor, alpha: Number, vec1: Tensor, vec2: Tensor) -> Tensor: ...
@overload
def addr(beta: Number, self: Tensor, alpha: Number, vec1: Tensor, vec2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def addr(beta: Number, self: Tensor, vec1: Tensor, vec2: Tensor) -> Tensor: ...
@overload
def addr(beta: Number, self: Tensor, vec1: Tensor, vec2: Tensor, *, out: Tensor) -> Tensor: ...
def affine_grid_generator(theta: Tensor, size: _size, align_corners: _bool) -> Tensor: ...
@overload
def all(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def all(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def all(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def allclose(self: Tensor, other: Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> _bool: ...
def alpha_dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...
def alpha_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...
def angle(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def any(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def any(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def any(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def arange(start: Number, end: Number, step: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def arange(start: Number, end: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def arange(end: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
def argmax(self: Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...
def argmin(self: Tensor, dim: Optional[_int]=None, keepdim: _bool=False) -> Tensor: ...
@overload
def argsort(self: Tensor, dim: _int=-1, descending: _bool=False) -> Tensor: ...
@overload
def argsort(self: Tensor, dim: Union[str, ellipsis, None], descending: _bool=False) -> Tensor: ...
def as_strided(self: Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...
def as_strided_(self: Tensor, size: _size, stride: _size, storage_offset: Optional[_int]=None) -> Tensor: ...
def as_tensor(data: Any, dtype: _dtype=None, device: Optional[_device]=None) -> Tensor: ...
def asin(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def asin_(self: Tensor) -> Tensor: ...
def asinh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def asinh_(self: Tensor) -> Tensor: ...
def atan(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def atan2(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def atan_(self: Tensor) -> Tensor: ...
def atanh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def atanh_(self: Tensor) -> Tensor: ...
def avg_pool1d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, ceil_mode: _bool=False, count_include_pad: _bool=True) -> Tensor: ...
@overload
def baddbmm(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor) -> Tensor: ...
@overload
def baddbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def bartlett_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def bartlett_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor: ...
def batch_norm_backward_elemt(grad_out: Tensor, input: Tensor, mean: Tensor, invstd: Tensor, weight: Optional[Tensor], mean_dy: Tensor, mean_dy_xmu: Tensor) -> Tensor: ...
def batch_norm_backward_reduce(grad_out: Tensor, input: Tensor, mean: Tensor, invstd: Tensor, weight: Optional[Tensor], input_g: _bool, weight_g: _bool, bias_g: _bool) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...
def batch_norm_elemt(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], mean: Tensor, invstd: Tensor, eps: _float, *, out: Optional[Tensor]=None) -> Tensor: ...
def batch_norm_gather_stats(input: Tensor, mean: Tensor, invstd: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float, eps: _float, count: _int) -> Tuple[Tensor, Tensor]: ...
def batch_norm_gather_stats_with_counts(input: Tensor, mean: Tensor, invstd: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float, eps: _float, counts: Tensor) -> Tuple[Tensor, Tensor]: ...
def batch_norm_stats(input: Tensor, eps: _float) -> Tuple[Tensor, Tensor]: ...
def batch_norm_update_stats(input: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float) -> Tuple[Tensor, Tensor]: ...
@overload
def bernoulli(self: Tensor, *, generator: Optional[Generator]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bernoulli(self: Tensor, p: _float, *, generator: Optional[Generator]=None, out: Optional[Tensor]=None) -> Tensor: ...
def bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor: ...
def bincount(self: Tensor, weights: Optional[Tensor]=None, minlength: _int=0) -> Tensor: ...
def binomial(count: Tensor, prob: Tensor, generator: Optional[Generator]=None) -> Tensor: ...
@overload
def bitwise_and(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bitwise_and(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def bitwise_not(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bitwise_or(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bitwise_or(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bitwise_xor(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bitwise_xor(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def blackman_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def blackman_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def bmm(self: Tensor, mat2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bucketize(self: Tensor, boundaries: Tensor, *, out_int32: _bool=False, right: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def bucketize(self: Number, boundaries: Tensor, *, out_int32: _bool=False, right: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...
def can_cast(from_: _dtype, to: _dtype) -> _bool: ...
@overload
def cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: Union[str, ellipsis, None], *, out: Optional[Tensor]=None) -> Tensor: ...
def ceil(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def ceil_(self: Tensor) -> Tensor: ...
def celu(self: Tensor, alpha: Number=1.0) -> Tensor: ...
def celu_(self: Tensor, alpha: Number=1.0) -> Tensor: ...
def channel_shuffle(self: Tensor, groups: _int) -> Tensor: ...
def cholesky(self: Tensor, upper: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
def cholesky_inverse(self: Tensor, upper: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
def cholesky_solve(self: Tensor, input2: Tensor, upper: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
def chunk(self: Tensor, chunks: _int, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...
def clamp(self, min: _float=-inf, max: _float=inf, *, out: Optional[Tensor]=None) -> Tensor: ...
def clamp_max(self: Tensor, max: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
def clamp_max_(self: Tensor, max: Number) -> Tensor: ...
def clamp_min(self: Tensor, min: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
def clamp_min_(self: Tensor, min: Number) -> Tensor: ...
def clone(self: Tensor, *, memory_format: Optional[memory_format]=None) -> Tensor: ...
def combinations(self: Tensor, r: _int=2, with_replacement: _bool=False) -> Tensor: ...
def conj(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def constant_pad_nd(self: Tensor, pad: _size, value: Number=0) -> Tensor: ...
def conv1d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...
def conv2d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...
def conv3d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...
def conv_tbc(self: Tensor, weight: Tensor, bias: Tensor, pad: _int=0) -> Tensor: ...
def conv_transpose1d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...
def conv_transpose2d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...
def conv_transpose3d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, output_padding: Union[_int, _size]=0, groups: _int=1, dilation: Union[_int, _size]=1) -> Tensor: ...
def convolution(input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size, groups: _int) -> Tensor: ...
def cos(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def cos_(self: Tensor) -> Tensor: ...
def cosh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def cosh_(self: Tensor) -> Tensor: ...
def cosine_similarity(x1: Tensor, x2: Tensor, dim: _int=1, eps: _float=1e-08) -> Tensor: ...
def cross(self: Tensor, other: Tensor, dim: Optional[_int]=None, *, out: Optional[Tensor]=None) -> Tensor: ...
def cudnn_affine_grid_generator(theta: Tensor, N: _int, C: _int, H: _int, W: _int) -> Tensor: ...
def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...
@overload
def cudnn_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
@overload
def cudnn_convolution(self: Tensor, weight: Tensor, padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
@overload
def cudnn_convolution_transpose(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
@overload
def cudnn_convolution_transpose(self: Tensor, weight: Tensor, padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def cudnn_grid_sampler(self: Tensor, grid: Tensor) -> Tensor: ...
def cudnn_is_acceptable(self: Tensor) -> _bool: ...
@overload
def cummax(self: Tensor, dim: _int, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def cummax(self: Tensor, dim: Union[str, ellipsis, None], *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def cummin(self: Tensor, dim: _int, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def cummin(self: Tensor, dim: Union[str, ellipsis, None], *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def cumprod(self: Tensor, dim: _int, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def cumprod(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def cumsum(self: Tensor, dim: _int, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def cumsum(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
def deg2rad(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def deg2rad_(self: Tensor) -> Tensor: ...
@overload
def dequantize(self: Tensor) -> Tensor: ...
@overload
def dequantize(tensors: Union[Tuple[Tensor, ...], List[Tensor]]) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...
def det(self: Tensor) -> Tensor: ...
def detach(self: Tensor) -> Tensor: ...
def detach_(self: Tensor) -> Tensor: ...
def diag(self: Tensor, diagonal: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...
def diag_embed(self: Tensor, offset: _int=0, dim1: _int=-2, dim2: _int=-1) -> Tensor: ...
def diagflat(self: Tensor, offset: _int=0) -> Tensor: ...
@overload
def diagonal(self: Tensor, offset: _int=0, dim1: _int=0, dim2: _int=1) -> Tensor: ...
@overload
def diagonal(self: Tensor, *, outdim: Union[str, ellipsis, None], dim1: Union[str, ellipsis, None], dim2: Union[str, ellipsis, None], offset: _int=0) -> Tensor: ...
def digamma(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def dist(self: Tensor, other: Tensor, p: Number=2) -> Tensor: ...
def div(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...
def dot(self: Tensor, tensor: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...
def dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...
def eig(self: Tensor, eigenvectors: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_eigenvalues_eigenvectors: ...
def embedding(weight: Tensor, indices: Tensor, padding_idx: _int=-1, scale_grad_by_freq: _bool=False, sparse: _bool=False) -> Tensor: ...
def embedding_bag(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: _bool=False, mode: _int=0, sparse: _bool=False, per_sample_weights: Optional[Tensor]=None, include_last_offset: _bool=False) -> Tuple[Tensor, Tensor, Tensor, Tensor]: ...
def embedding_renorm_(self: Tensor, indices: Tensor, max_norm: _float, norm_type: _float) -> Tensor: ...
@overload
def empty(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def empty(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def empty(size: _size, *, memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def empty(*size: _int, memory_format: Optional[memory_format]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def empty_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def empty_meta(size: _size, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def empty_meta(*size: _int, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def empty_quantized(size: _size, qtensor: Tensor) -> Tensor: ...
def empty_strided(size: _size, stride: _size, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def eq(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def eq(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def equal(self: Tensor, other: Tensor) -> _bool: ...
def erf(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def erf_(self: Tensor) -> Tensor: ...
def erfc(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def erfc_(self: Tensor) -> Tensor: ...
def erfinv(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def exp(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def exp_(self: Tensor) -> Tensor: ...
def expm1(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def expm1_(self: Tensor) -> Tensor: ...
@overload
def eye(n: _int, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def eye(n: _int, m: _int, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def fake_quantize_per_channel_affine(self: Tensor, scale: Tensor, zero_point: Tensor, axis: _int, quant_min: _int, quant_max: _int) -> Tensor: ...
def fake_quantize_per_tensor_affine(self: Tensor, scale: _float, zero_point: _int, quant_min: _int, quant_max: _int) -> Tensor: ...
def fbgemm_linear_fp16_weight(input: Tensor, packed_weight: Tensor, bias: Tensor) -> Tensor: ...
def fbgemm_linear_fp16_weight_fp32_activation(input: Tensor, packed_weight: Tensor, bias: Tensor) -> Tensor: ...
def fbgemm_linear_int8_weight(input: Tensor, weight: Tensor, packed: Tensor, col_offsets: Tensor, weight_scale: Number, weight_zero_point: Number, bias: Tensor) -> Tensor: ...
def fbgemm_linear_int8_weight_fp32_activation(input: Tensor, weight: Tensor, packed: Tensor, col_offsets: Tensor, weight_scale: Number, weight_zero_point: Number, bias: Tensor) -> Tensor: ...
def fbgemm_linear_quantize_weight(input: Tensor) -> Tuple[Tensor, Tensor, _float, _int]: ...
def fbgemm_pack_gemm_matrix_fp16(input: Tensor) -> Tensor: ...
@overload
def fbgemm_pack_quantized_matrix(input: Tensor) -> Tensor: ...
@overload
def fbgemm_pack_quantized_matrix(input: Tensor, K: _int, N: _int) -> Tensor: ...
def feature_alpha_dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...
def feature_alpha_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...
def feature_dropout(input: Tensor, p: _float, train: _bool) -> Tensor: ...
def feature_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor: ...
def fft(self: Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...
@overload
def fill_(self: Tensor, value: Number) -> Tensor: ...
@overload
def fill_(self: Tensor, value: Tensor) -> Tensor: ...
@overload
def flatten(self: Tensor, start_dim: _int=0, end_dim: _int=-1) -> Tensor: ...
@overload
def flatten(self: Tensor, start_dim: _int, end_dim: _int, out_dim: Union[str, ellipsis, None]) -> Tensor: ...
@overload
def flatten(self: Tensor, start_dim: Union[str, ellipsis, None], end_dim: Union[str, ellipsis, None], out_dim: Union[str, ellipsis, None]) -> Tensor: ...
@overload
def flatten(self: Tensor, dims: Sequence[Union[str, ellipsis, None]], out_dim: Union[str, ellipsis, None]) -> Tensor: ...
def flip(self: Tensor, dims: _size) -> Tensor: ...
def fliplr(self: Tensor) -> Tensor: ...
def flipud(self: Tensor) -> Tensor: ...
def floor(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def floor_(self: Tensor) -> Tensor: ...
def floor_divide(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def fmod(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def fmod(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def frac(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def frac_(self: Tensor) -> Tensor: ...
@overload
def frobenius_norm(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def frobenius_norm(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
def from_file(filename: str, shared: Optional[_bool]=None, size: Optional[_int]=0, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def from_numpy(ndarray) -> Tensor: ...
@overload
def full(size: _size, fill_value: Number, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def full(size: _size, fill_value: Number, *, names: List[Union[str, None]], dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
def full_like(self: Tensor, fill_value: Number, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def gather(self: Tensor, dim: _int, index: Tensor, *, sparse_grad: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def gather(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, *, sparse_grad: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def ge(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def ge(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def geqrf(self: Tensor, *, out: Optional[Tensor]=None) -> namedtuple_a_tau: ...
def ger(self: Tensor, vec2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def get_default_dtype() -> _dtype: ...
def get_num_interop_threads() -> _int: ...
def get_num_threads() -> _int: ...
def grid_sampler(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor: ...
def grid_sampler_2d(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor: ...
def grid_sampler_3d(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor: ...
def group_norm(input: Tensor, num_groups: _int, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, eps: _float=1e-05, cudnn_enabled: _bool=True) -> Tensor: ...
@overload
def gru(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...
@overload
def gru(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]: ...
def gru_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tensor: ...
@overload
def gt(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def gt(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def hamming_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hamming_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hamming_window(window_length: _int, periodic: _bool, alpha: _float, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hamming_window(window_length: _int, periodic: _bool, alpha: _float, beta: _float, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hann_window(window_length: _int, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def hann_window(window_length: _int, periodic: _bool, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def hardshrink(self: Tensor, lambd: Number=0.5) -> Tensor: ...
def histc(self: Tensor, bins: _int=100, min: Number=0, max: Number=0, *, out: Optional[Tensor]=None) -> Tensor: ...
def hspmm(mat1: Tensor, mat2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def ifft(self: Tensor, signal_ndim: _int, normalized: _bool=False) -> Tensor: ...
def imag(self: Tensor) -> Tensor: ...
@overload
def index_add(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...
@overload
def index_add(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor) -> Tensor: ...
@overload
def index_copy(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...
@overload
def index_copy(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor) -> Tensor: ...
@overload
def index_fill(self: Tensor, dim: _int, index: Tensor, value: Number) -> Tensor: ...
@overload
def index_fill(self: Tensor, dim: _int, index: Tensor, value: Tensor) -> Tensor: ...
@overload
def index_fill(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, value: Number) -> Tensor: ...
@overload
def index_fill(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, value: Tensor) -> Tensor: ...
def index_put(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False) -> Tensor: ...
def index_put_(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool=False) -> Tensor: ...
@overload
def index_select(self: Tensor, dim: _int, index: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def index_select(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def init_num_threads() -> None: ...
def instance_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], use_input_stats: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor: ...
def int_repr(self: Tensor) -> Tensor: ...
def inverse(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def irfft(self: Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True, signal_sizes: _size=()) -> Tensor: ...
def is_complex(self: Tensor) -> _bool: ...
def is_distributed(self: Tensor) -> _bool: ...
def is_floating_point(self: Tensor) -> _bool: ...
def is_grad_enabled() -> _bool: ...
def is_nonzero(self: Tensor) -> _bool: ...
def is_same_size(self: Tensor, other: Tensor) -> _bool: ...
def is_signed(self: Tensor) -> _bool: ...
def is_vulkan_available() -> _bool: ...
def isclose(self: Tensor, other: Tensor, rtol: _float=1e-05, atol: _float=1e-08, equal_nan: _bool=False) -> Tensor: ...
def isfinite(self: Tensor) -> Tensor: ...
def isinf(self: Tensor) -> Tensor: ...
def isnan(self: Tensor) -> Tensor: ...
@overload
def kthvalue(self: Tensor, k: _int, dim: _int=-1, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def kthvalue(self: Tensor, k: _int, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
def layer_norm(input: Tensor, normalized_shape: _size, weight: Optional[Tensor]=None, bias: Optional[Tensor]=None, eps: _float=1e-05, cudnn_enable: _bool=True) -> Tensor: ...
@overload
def le(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def le(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def lerp(self: Tensor, end: Tensor, weight: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def lerp(self: Tensor, end: Tensor, weight: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def lgamma(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def linspace(start: Number, end: Number, steps: _int=100, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def log(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def log10(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def log10_(self: Tensor) -> Tensor: ...
def log1p(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def log1p_(self: Tensor) -> Tensor: ...
def log2(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def log2_(self: Tensor) -> Tensor: ...
def log_(self: Tensor) -> Tensor: ...
@overload
def log_softmax(self: Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
@overload
def log_softmax(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...
def logaddexp(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def logaddexp2(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def logcumsumexp(self: Tensor, dim: _int, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def logcumsumexp(self: Tensor, dim: Union[str, ellipsis, None], *, out: Optional[Tensor]=None) -> Tensor: ...
def logdet(self: Tensor) -> Tensor: ...
def logical_and(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def logical_not(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def logical_or(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def logical_xor(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def logspace(start: Number, end: Number, steps: _int=100, base: _float=10.0, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def logsumexp(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def logsumexp(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def lstm(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor, Tensor]: ...
@overload
def lstm(data: Tensor, batch_sizes: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor, Tensor]: ...
def lstm_cell(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tuple[Tensor, Tensor]: ...
def lstsq(self: Tensor, A: Tensor, *, out: Optional[Tensor]=None) -> namedtuple_solution_QR: ...
@overload
def lt(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def lt(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def lu_solve(self: Tensor, LU_data: Tensor, LU_pivots: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def masked_fill(self: Tensor, mask: Tensor, value: Number) -> Tensor: ...
@overload
def masked_fill(self: Tensor, mask: Tensor, value: Tensor) -> Tensor: ...
def masked_scatter(self: Tensor, mask: Tensor, source: Tensor) -> Tensor: ...
def masked_select(self: Tensor, mask: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def matmul(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def matrix_power(self: Tensor, n: _int) -> Tensor: ...
@overload
def matrix_rank(self: Tensor, tol: _float, symmetric: _bool=False) -> Tensor: ...
@overload
def matrix_rank(self: Tensor, symmetric: _bool=False) -> Tensor: ...
@overload
def max(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def max(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def max(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def max(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def max_pool1d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
def max_pool1d_with_indices(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tuple[Tensor, Tensor]: ...
def max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
def max_pool3d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
@overload
def mean(self: Tensor, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def mean(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def mean(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def median(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def median(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def median(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def min(self: Tensor, dim: _int, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def min(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def min(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def min(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def miopen_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[Tensor, Tensor, Tensor]: ...
def miopen_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def miopen_convolution_transpose(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def miopen_depthwise_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor: ...
def miopen_rnn(input: Tensor, weight: Union[Tuple[Tensor, ...], List[Tensor]], weight_stride0: _int, hx: Tensor, cx: Optional[Tensor], mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, dropout: _float, train: _bool, bidirectional: _bool, batch_sizes: _size, dropout_state: Optional[Tensor]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def mkldnn_adaptive_avg_pool2d(self: Tensor, output_size: Union[_int, _size]) -> Tensor: ...
def mkldnn_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int) -> Tensor: ...
def mkldnn_convolution_backward_weights(weight_size: _size, grad_output: Tensor, self: Tensor, padding: _size, stride: _size, dilation: _size, groups: _int, bias_defined: _bool) -> Tuple[Tensor, Tensor]: ...
def mkldnn_max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
def mm(self: Tensor, mat2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def mode(self: Tensor, dim: _int=-1, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def mode(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
def mul(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...
def multinomial(self: Tensor, num_samples: _int, replacement: _bool=False, *, generator: Optional[Generator]=None, out: Optional[Tensor]=None) -> Tensor: ...
def mv(self: Tensor, vec: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def mvlgamma(self: Tensor, p: _int) -> Tensor: ...
@overload
def narrow(self: Tensor, dim: _int, start: _int, length: _int) -> Tensor: ...
@overload
def narrow(self: Tensor, dim: _int, start: Tensor, length: _int) -> Tensor: ...
def native_batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, *, out: Optional[Tensor]=None) -> Tuple[Tensor, Tensor, Tensor]: ...
def native_group_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], N: _int, C: _int, HxW: _int, group: _int, eps: _float) -> Tuple[Tensor, Tensor, Tensor]: ...
def native_layer_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], M: _int, N: _int, eps: _float) -> Tuple[Tensor, Tensor, Tensor]: ...
def native_norm(self: Tensor, p: Number=2) -> Tensor: ...
@overload
def ne(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def ne(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def neg(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def neg_(self: Tensor) -> Tensor: ...
@overload
def nonzero(input: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def nonzero(input: Tensor, *, as_tuple: bool=...) -> Tensor: ...
def norm_except_dim(v: Tensor, pow: _int=2, dim: _int=0) -> Tensor: ...
@overload
def normal(mean: Tensor, std: _float=1, *, generator: Optional[Generator]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def normal(mean: _float, std: Tensor, *, generator: Optional[Generator]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def normal(mean: Tensor, std: Tensor, *, generator: Optional[Generator]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def normal(mean: _float, std: _float, size: _size, *, generator: Optional[Generator]=None, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def nuclear_norm(self: Tensor, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def nuclear_norm(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
def numel(self: Tensor) -> _int: ...
@overload
def ones(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def ones(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def ones(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def ones(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def ones_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def orgqr(self: Tensor, input2: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def ormqr(self: Tensor, input2: Tensor, input3: Tensor, left: _bool=True, transpose: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
def pairwise_distance(x1: Tensor, x2: Tensor, p: _float=2, eps: _float=1e-06, keepdim: _bool=False) -> Tensor: ...
def pdist(self: Tensor, p: _float=2) -> Tensor: ...
def pinverse(self: Tensor, rcond: _float=1e-15) -> Tensor: ...
def pixel_shuffle(self: Tensor, upscale_factor: _int) -> Tensor: ...
def poisson(self: Tensor, generator: Optional[Generator]=None) -> Tensor: ...
def poisson_nll_loss(input: Tensor, target: Tensor, log_input: _bool, full: _bool, eps: _float, reduction: _int) -> Tensor: ...
def polygamma(n: _int, self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def pow(self: Tensor, exponent: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def pow(self: Tensor, exponent: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def pow(self: Number, exponent: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def prelu(self: Tensor, weight: Tensor) -> Tensor: ...
@overload
def prod(self: Tensor, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def prod(self: Tensor, dim: _int, keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def prod(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
def promote_types(type1: _dtype, type2: _dtype) -> _dtype: ...
def q_per_channel_axis(self: Tensor) -> _int: ...
def q_per_channel_scales(self: Tensor) -> Tensor: ...
def q_per_channel_zero_points(self: Tensor) -> Tensor: ...
def q_scale(self: Tensor) -> _float: ...
def q_zero_point(self: Tensor) -> _int: ...
def qr(self: Tensor, some: _bool=True, *, out: Optional[Tensor]=None) -> namedtuple_Q_R: ...
def quantize_per_channel(self: Tensor, scales: Tensor, zero_points: Tensor, axis: _int, dtype: _dtype) -> Tensor: ...
@overload
def quantize_per_tensor(self: Tensor, scale: _float, zero_point: _int, dtype: _dtype) -> Tensor: ...
@overload
def quantize_per_tensor(tensors: Union[Tuple[Tensor, ...], List[Tensor]], scales: Tensor, zero_points: Tensor, dtype: _dtype) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...
def quantized_batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], mean: Tensor, var: Tensor, eps: _float, output_scale: _float, output_zero_point: _int) -> Tensor: ...
def quantized_gru_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...
def quantized_lstm_cell(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tuple[Tensor, Tensor]: ...
def quantized_max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size]=(), padding: Union[_int, _size]=0, dilation: Union[_int, _size]=1, ceil_mode: _bool=False) -> Tensor: ...
def quantized_rnn_relu_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...
def quantized_rnn_tanh_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor: ...
def rad2deg(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def rad2deg_(self: Tensor) -> Tensor: ...
@overload
def rand(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(size: _size, *, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(*size: _int, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(size: _size, *, generator: Optional[Generator], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def rand(*size: _int, generator: Optional[Generator], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def rand_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randint(low: _int, high: _int, size: _size, *, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def randint(high: _int, size: _size, *, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
@overload
def randint_like(self: Tensor, high: _int, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randint_like(self: Tensor, low: _int, high: _int, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(size: _size, *, generator: Optional[Generator], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(*size: _int, generator: Optional[Generator], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(size: _size, *, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randn(*size: _int, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def randn_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randperm(n: _int, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def randperm(n: _int, *, generator: Optional[Generator], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def range(start: Number, end: Number, step: Number=1, *, out: Optional[Tensor]=None, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
def real(self: Tensor) -> Tensor: ...
def reciprocal(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def reciprocal_(self: Tensor) -> Tensor: ...
def relu(self: Tensor) -> Tensor: ...
def relu_(self: Tensor) -> Tensor: ...
@overload
def remainder(self: Tensor, other: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def remainder(self: Tensor, other: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def renorm(self: Tensor, p: Number, dim: _int, maxnorm: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def repeat_interleave(repeats: Tensor) -> Tensor: ...
@overload
def repeat_interleave(self: Tensor, repeats: Tensor, dim: Optional[_int]=None) -> Tensor: ...
@overload
def repeat_interleave(self: Tensor, repeats: _int, dim: Optional[_int]=None) -> Tensor: ...
def reshape(self: Tensor, shape: _size) -> Tensor: ...
def resize_as_(self: Tensor, the_template: Tensor, *, memory_format: Optional[memory_format]=None) -> Tensor: ...
@overload
def result_type(tensor: Tensor, other: Tensor) -> _dtype: ...
@overload
def result_type(tensor: Tensor, other: Number) -> _dtype: ...
@overload
def result_type(scalar: Number, tensor: Tensor) -> _dtype: ...
@overload
def result_type(scalar1: Number, scalar2: Number) -> _dtype: ...
def rfft(self: Tensor, signal_ndim: _int, normalized: _bool=False, onesided: _bool=True) -> Tensor: ...
@overload
def rnn_relu(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...
@overload
def rnn_relu(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]: ...
def rnn_relu_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tensor: ...
@overload
def rnn_tanh(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]: ...
@overload
def rnn_tanh(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]: ...
def rnn_tanh_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor]=None, b_hh: Optional[Tensor]=None) -> Tensor: ...
def roll(self: Tensor, shifts: Union[_int, _size], dims: Union[_int, _size]=()) -> Tensor: ...
def rot90(self: Tensor, k: _int=1, dims: _size=(0,1)) -> Tensor: ...
def round(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def round_(self: Tensor) -> Tensor: ...
def rrelu(self: Tensor, lower: Number=0.125, upper: Number=0.3333333333333333, training: _bool=False, generator: Optional[Generator]=None) -> Tensor: ...
def rrelu_(self: Tensor, lower: Number=0.125, upper: Number=0.3333333333333333, training: _bool=False, generator: Optional[Generator]=None) -> Tensor: ...
def rsqrt(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def rsqrt_(self: Tensor) -> Tensor: ...
@overload
def rsub(self: Tensor, other: Tensor, *, alpha: Number=1) -> Tensor: ...
@overload
def rsub(self: Tensor, other: Number, alpha: Number=1) -> Tensor: ...
def scalar_tensor(s: Number, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def scatter(self: Tensor, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...
@overload
def scatter(self: Tensor, dim: _int, index: Tensor, value: Number) -> Tensor: ...
@overload
def scatter(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor) -> Tensor: ...
@overload
def scatter(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, value: Number) -> Tensor: ...
@overload
def scatter_add(self: Tensor, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...
@overload
def scatter_add(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor) -> Tensor: ...
@overload
def searchsorted(sorted_sequence: Tensor, self: Tensor, *, out_int32: _bool=False, right: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def searchsorted(sorted_sequence: Tensor, self: Number, *, out_int32: _bool=False, right: _bool=False, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def select(self: Tensor, dim: Union[str, ellipsis, None], index: _int) -> Tensor: ...
@overload
def select(self: Tensor, dim: _int, index: _int) -> Tensor: ...
def selu(self: Tensor) -> Tensor: ...
def selu_(self: Tensor) -> Tensor: ...
def set_flush_denormal(mode: _bool) -> _bool: ...
def set_num_interop_threads(num: _int) -> None: ...
def set_num_threads(num: _int) -> None: ...
def sigmoid(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def sigmoid_(self: Tensor) -> Tensor: ...
def sign(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def sin(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def sin_(self: Tensor) -> Tensor: ...
def sinh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def sinh_(self: Tensor) -> Tensor: ...
def slogdet(self: Tensor) -> namedtuple_sign_logabsdet: ...
def smm(self: Tensor, mat2: Tensor) -> Tensor: ...
@overload
def softmax(self: Tensor, dim: _int, dtype: Optional[_dtype]=None) -> Tensor: ...
@overload
def softmax(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype]=None) -> Tensor: ...
def solve(self: Tensor, A: Tensor, *, out: Optional[Tensor]=None) -> namedtuple_solution_LU: ...
@overload
def sort(self: Tensor, dim: _int=-1, descending: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
@overload
def sort(self: Tensor, dim: Union[str, ellipsis, None], descending: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
def sparse_coo_tensor(indices: Tensor, values: Union[Tensor,List], size: Optional[_size]=None, *, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def split_with_sizes(self: Tensor, split_sizes: _size, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...
def sqrt(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def sqrt_(self: Tensor) -> Tensor: ...
def square(self: Tensor) -> Tensor: ...
def square_(self: Tensor) -> Tensor: ...
@overload
def squeeze(self: Tensor) -> Tensor: ...
@overload
def squeeze(self: Tensor, dim: _int) -> Tensor: ...
@overload
def squeeze(self: Tensor, dim: Union[str, ellipsis, None]) -> Tensor: ...
@overload
def sspaddmm(self: Tensor, mat1: Tensor, mat2: Tensor, *, beta: Number=1, alpha: Number=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def sspaddmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor) -> Tensor: ...
@overload
def sspaddmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor) -> Tensor: ...
def stack(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def std(self: Tensor, unbiased: _bool=True, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def std(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def std(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def std_mean(self: Tensor, unbiased: _bool=True) -> Tuple[Tensor, Tensor]: ...
@overload
def std_mean(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...
@overload
def std_mean(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...
@overload
def sub(input: Union[Tensor, Number], other: Union[Tensor, Number], *, alpha: Optional[Number]=1, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def sub(self: Tensor, alpha: Number, other: Tensor) -> Tensor: ...
@overload
def sub(self: Tensor, alpha: Number, other: Tensor, *, out: Tensor) -> Tensor: ...
@overload
def sum(self: Tensor, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def sum(self: Tensor, dim: Union[_int, _size], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def sum(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool=False, *, dtype: Optional[_dtype]=None, out: Optional[Tensor]=None) -> Tensor: ...
def svd(self: Tensor, some: _bool=True, compute_uv: _bool=True, *, out: Optional[Tensor]=None) -> namedtuple_U_S_V: ...
def symeig(self: Tensor, eigenvectors: _bool=False, upper: _bool=True, *, out: Optional[Tensor]=None) -> namedtuple_eigenvalues_eigenvectors: ...
def t(self: Tensor) -> Tensor: ...
def take(self: Tensor, index: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def tan(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def tan_(self: Tensor) -> Tensor: ...
def tanh(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def tanh_(self: Tensor) -> Tensor: ...
def tensor(data: Any, dtype: Optional[_dtype]=None, device: Union[_device, str, None]=None, requires_grad: _bool=False) -> Tensor: ...
def threshold(self: Tensor, threshold: Number, value: Number, *, out: Optional[Tensor]=None) -> Tensor: ...
def threshold_(self: Tensor, threshold: Number, value: Number) -> Tensor: ...
def topk(self: Tensor, k: _int, dim: _int=-1, largest: _bool=True, sorted: _bool=True, *, out: Optional[Tensor]=None) -> namedtuple_values_indices: ...
def trace(self: Tensor) -> Tensor: ...
@overload
def transpose(self: Tensor, dim0: _int, dim1: _int) -> Tensor: ...
@overload
def transpose(self: Tensor, dim0: Union[str, ellipsis, None], dim1: Union[str, ellipsis, None]) -> Tensor: ...
@overload
def trapz(y: Tensor, x: Tensor, *, dim: _int=-1) -> Tensor: ...
@overload
def trapz(y: Tensor, *, dx: _float=1, dim: _int=-1) -> Tensor: ...
def triangular_solve(self: Tensor, A: Tensor, upper: _bool=True, transpose: _bool=False, unitriangular: _bool=False, *, out: Optional[Tensor]=None) -> namedtuple_solution_cloned_coefficient: ...
def tril(self: Tensor, diagonal: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...
def tril_indices(row: _int, col: _int, offset: _int=0, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def triu(self: Tensor, diagonal: _int=0, *, out: Optional[Tensor]=None) -> Tensor: ...
def triu_indices(row: _int, col: _int, offset: _int=0, *, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def true_divide(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...
def trunc(self: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...
def trunc_(self: Tensor) -> Tensor: ...
@overload
def unbind(self: Tensor, dim: _int=0) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...
@overload
def unbind(self: Tensor, dim: Union[str, ellipsis, None]) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...
def unique_dim(self: Tensor, dim: _int, sorted: _bool=True, return_inverse: _bool=False, return_counts: _bool=False) -> Tuple[Tensor, Tensor, Tensor]: ...
def unsqueeze(self: Tensor, dim: _int) -> Tensor: ...
def vander(x: Tensor, N: Optional[_int]=None, increasing: _bool=False) -> Tensor: ...
@overload
def var(self: Tensor, unbiased: _bool=True, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def var(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def var(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool=True, keepdim: _bool=False, *, out: Optional[Tensor]=None) -> Tensor: ...
@overload
def var_mean(self: Tensor, unbiased: _bool=True) -> Tuple[Tensor, Tensor]: ...
@overload
def var_mean(self: Tensor, dim: Union[_int, _size], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...
@overload
def var_mean(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool=True, keepdim: _bool=False) -> Tuple[Tensor, Tensor]: ...
def view_as_complex(self: Tensor) -> Tensor: ...
def view_as_real(self: Tensor) -> Tensor: ...
@overload
def where(condition: Tensor, self: Tensor, other: Tensor) -> Tensor: ...
@overload
def where(condition: Tensor) -> Union[Tuple[Tensor, ...], List[Tensor]]: ...
def zero_(self: Tensor) -> Tensor: ...
@overload
def zeros(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def zeros(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def zeros(size: _size, *, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
@overload
def zeros(*size: _int, out: Optional[Tensor]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
def zeros_like(self: Tensor, *, memory_format: Optional[memory_format]=None, dtype: _dtype=None, layout: _layout=strided, device: Union[_device, str, None]=None, requires_grad:_bool=False) -> Tensor: ...
